#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/moonshine_streaming/modular_moonshine_streaming.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_moonshine_streaming.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2026 the HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from collections.abc import Callable
from typing import Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache
from ...generation import GenerationMixin
from ...integrations import use_kernelized_func
from ...modeling_attn_mask_utils import (
    _prepare_4d_attention_mask,
    _prepare_4d_attention_mask_for_sdpa,
    _prepare_4d_causal_attention_mask,
    _prepare_4d_causal_attention_mask_for_sdpa,
)
from ...modeling_outputs import (
    BaseModelOutput,
    BaseModelOutputWithPastAndCrossAttentions,
    Seq2SeqLMOutput,
    Seq2SeqModelOutput,
)
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging
from .configuration_moonshine_streaming import MoonshineStreamingConfig


logger = logging.get_logger(__name__)


class MoonshineStreamingFrameCMVN(nn.Module):
    def __init__(self, eps: float = 1e-6):
        super().__init__()
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mean = x.mean(dim=-1, keepdim=True)
        centered = x - mean
        rms = (centered.pow(2).mean(dim=-1, keepdim=True) + self.eps).sqrt()
        return centered / rms


class MoonshineStreamingAsinhCompress(nn.Module):
    def __init__(self, k_init: float = 0.75):
        super().__init__()
        self.log_k = nn.Parameter(torch.log(torch.tensor(k_init)))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.asinh(torch.exp(self.log_k) * x)


class MoonshineStreamingCausalConv1d(nn.Module):
    def __init__(
        self,
        c_in: int,
        c_out: int,
        kernel: int,
        stride: int = 1,
        dilation: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.left_pad = (kernel - 1) * dilation
        self.conv = nn.utils.parametrizations.weight_norm(
            nn.Conv1d(c_in, c_out, kernel, stride=stride, dilation=dilation, bias=bias)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.pad(x, (self.left_pad, 0))
        return self.conv(x)


class MoonshineStreamingRotaryEmbedding(nn.Module):
    def __init__(
        self,
        dim: int,
        base: float = 10000.0,
        interpolation_factor: float = 1.0,
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device, dtype=dtype or torch.float32) / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.interpolation_factor = interpolation_factor

    def forward(
        self,
        seq_len: Optional[int],
        device: torch.device,
        positions: Optional[Tensor] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> tuple[Tensor, Tensor]:
        """Returns (cos, sin) tuple for position embeddings with shape (1, seq_len, dim)."""
        if positions is None:
            if seq_len is None:
                raise ValueError("seq_len must be provided when positions is None.")
            positions = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)
        else:
            positions = positions.to(device=device, dtype=self.inv_freq.dtype).view(-1)

        freqs = torch.einsum("i,j->ij", positions, self.inv_freq.to(device)) / self.interpolation_factor
        # Concatenate freqs to match expected format for apply_rotary_pos_emb
        # apply_rotary_pos_emb will handle the interleaving internally
        emb = torch.cat((freqs, freqs), dim=-1)
        cos = emb.cos()
        sin = emb.sin()
        # Return with batch dimension for broadcasting: (1, seq_len, dim)
        # Cast to requested dtype if specified
        if dtype is not None:
            cos = cos.to(dtype=dtype)
            sin = sin.to(dtype=dtype)
        return cos.unsqueeze(0), sin.unsqueeze(0)


class MoonshineStreamingLayerNorm(nn.Module):
    def __init__(self, dim: int, unit_offset: bool = True, device=None, dtype=None):
        super().__init__()
        self.unit_offset = float(unit_offset)
        self.ln = nn.LayerNorm(dim, elementwise_affine=False, device=device, dtype=dtype)
        self.gamma = nn.Parameter(torch.ones(dim, device=device, dtype=dtype))
        nn.init.constant_(self.gamma, 1.0 - self.unit_offset)

    def forward(self, x: Tensor) -> Tensor:
        normed = self.ln(x)
        gamma = self.gamma + self.unit_offset
        return normed * gamma


class MoonshineStreamingSwiGLU(nn.Module):
    def __init__(self, dim_in: int, dim_out: int, device=None, dtype=None):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2, device=device, dtype=dtype)

    def forward(self, x: Tensor) -> Tensor:
        x, gate = self.proj(x).chunk(2, dim=-1)
        return x * F.silu(gate)


class MoonshineStreamingFeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        mult: int = 4,
        use_swiglu: bool = True,
        dropout: float = 0.1,
        device=None,
        dtype=None,
    ):
        super().__init__()
        inner_dim = dim * mult
        if use_swiglu:
            self.project_in = MoonshineStreamingSwiGLU(dim, inner_dim, device=device, dtype=dtype)
        else:
            self.project_in = nn.Sequential(nn.Linear(dim, inner_dim, device=device, dtype=dtype), nn.GELU())

        self.dropout = nn.Dropout(dropout)
        self.project_out = nn.Linear(inner_dim, dim, bias=True, device=device, dtype=dtype)

    def forward(self, x: Tensor) -> Tensor:
        x = self.project_in(x)
        x = self.dropout(x)
        return self.project_out(x)


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., 0::2]
    x2 = x[..., 1::2]
    return torch.stack((-x2, x1), dim=-1).flatten(-2)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)

    # Interleave them instead of usual shape
    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)
    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)

    # Keep half or full tensor for later concatenation
    rotary_dim = cos.shape[-1]
    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]
    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]

    # Apply rotary embeddings on the first half or full tensor
    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)
    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)

    # Concatenate back to full shape
    q_embed = torch.cat([q_embed, q_pass], dim=-1)
    k_embed = torch.cat([k_embed, k_pass], dim=-1)
    return q_embed, k_embed


@use_kernelized_func(apply_rotary_pos_emb)
class MoonshineStreamingAttention(nn.Module):
    """Attention for MoonshineStreaming, inheriting from MoonshineAttention.

    Adds support for explicit `dim` parameter to handle different encoder/decoder dimensions.
    Also handles the case where position_embeddings is None (no rotary embeddings).
    """

    def __init__(
        self,
        config: MoonshineStreamingConfig,
        layer_idx: int,
        is_causal: bool,
        num_attention_heads: int,
        num_key_value_heads: int,
        dim: Optional[int] = None,
    ):
        super().__init__()
        # If dim is specified and different from hidden_size, temporarily modify config
        # so that projections use the correct dimension
        original_hidden_size = config.hidden_size
        if dim is not None and dim != original_hidden_size:
            config.update({"hidden_size": dim})
        config.update({"num_attention_heads": num_attention_heads, "num_key_value_heads": num_key_value_heads})
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = is_causal

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)

        # Pad head dimension to the next specified multiple.
        if self.config.pad_head_dim_to_multiple_of is not None:
            target_multiple = self.config.pad_head_dim_to_multiple_of
            target_head_dim = target_multiple * ((self.head_dim + target_multiple - 1) // target_multiple)
            self.head_dim_padding = target_head_dim - self.head_dim
        else:
            self.head_dim_padding = 0

        # Restore original hidden_size
        if dim is not None and dim != original_hidden_size:
            config.update({"hidden_size": original_hidden_size})

    def forward(
        self,
        hidden_states: Tensor,
        position_embeddings: Optional[tuple[Tensor, Tensor]] = None,
        attention_mask: Optional[Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
        key_value_states: Optional[Tensor] = None,
        **kwargs,
    ) -> tuple[Tensor, Optional[Tensor]]:
        # MoonshineStreaming encoder has rotary_dim=0 by default (no rotary embeddings).
        # MoonshineAttention expects position_embeddings for self-attention, so we provide
        # identity embeddings (cos=1, sin=0) which result in no rotation being applied.
        # The overhead is negligible: O(seq_len * head_dim) vs O(seq_len^2 * head_dim) for attention.
        if position_embeddings is None and key_value_states is None:
            seq_len = hidden_states.shape[1]
            device = hidden_states.device
            dtype = hidden_states.dtype
            # Shape (1, seq_len, head_dim) to match MoonshineStreamingRotaryEmbedding output format
            dummy_cos = torch.ones(1, seq_len, self.head_dim, device=device, dtype=dtype)
            dummy_sin = torch.zeros(1, seq_len, self.head_dim, device=device, dtype=dtype)
            position_embeddings = (dummy_cos, dummy_sin)
        bsz, q_len = hidden_states.shape[:-1]

        query_states = (
            self.q_proj(hidden_states).view(bsz, q_len, self.config.num_key_value_heads, self.head_dim).transpose(1, 2)
        )

        is_cross_attention = key_value_states is not None
        if past_key_values is not None:
            is_updated = past_key_values.is_updated.get(self.layer_idx)
            if is_cross_attention:
                # after the first generated id, we can subsequently re-use all key/value_states from cache
                past_key_values.is_updated[self.layer_idx] = True
                past_key_values = past_key_values.cross_attention_cache
            else:
                past_key_values = past_key_values.self_attention_cache

        # use key_value_states if cross attention
        current_states = key_value_states if key_value_states is not None else hidden_states
        if is_cross_attention and past_key_values and is_updated:
            key_states = past_key_values.layers[self.layer_idx].keys
            value_states = past_key_values.layers[self.layer_idx].values
        else:
            key_states = (
                self.k_proj(current_states)
                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)
                .transpose(1, 2)
            )
            value_states = (
                self.v_proj(current_states)
                .view(bsz, -1, self.config.num_key_value_heads, self.head_dim)
                .transpose(1, 2)
            )
            if is_cross_attention and past_key_values is not None:
                key_states, value_states = past_key_values.update(
                    key_states, value_states, self.layer_idx, {"cache_position": cache_position}
                )

        if not is_cross_attention:
            cos, sin = position_embeddings
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

            if past_key_values is not None:
                cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
                key_states, value_states = past_key_values.update(
                    key_states, value_states, self.layer_idx, cache_kwargs
                )

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        is_causal = self.is_causal and attention_mask is None and q_len > 1

        if self.head_dim_padding > 0:
            query_states = torch.nn.functional.pad(query_states, (0, self.head_dim_padding))
            key_states = torch.nn.functional.pad(key_states, (0, self.head_dim_padding))
            value_states = torch.nn.functional.pad(value_states, (0, self.head_dim_padding))

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            is_causal=is_causal,
            **kwargs,
        )

        if self.head_dim_padding > 0:
            attn_output = attn_output[..., : -self.head_dim_padding]

        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class MoonshineStreamingEncoderLayer(nn.Module):
    """Encoder layer with flat structure matching Moonshine convention."""

    def __init__(self, config: MoonshineStreamingConfig, layer_idx: int):
        super().__init__()
        self.config = config

        self.self_attn = MoonshineStreamingAttention(
            config=config,
            layer_idx=layer_idx,
            is_causal=False,
            num_attention_heads=config.encoder_num_attention_heads,
            num_key_value_heads=config.encoder_num_attention_heads,
            dim=config.encoder_dim,
        )
        self.mlp = MoonshineStreamingFeedForward(
            config.encoder_dim,
            mult=config.ffn_mult,
            use_swiglu=config.use_swiglu_encoder,
            dropout=config.ff_dropout,
        )
        self.input_layernorm = MoonshineStreamingLayerNorm(config.encoder_dim)
        self.post_attention_layernorm = MoonshineStreamingLayerNorm(config.encoder_dim)

    def forward(
        self,
        hidden_states: Tensor,
        position_embeddings: Optional[tuple[Tensor, Tensor]] = None,
        attention_mask: Optional[Tensor] = None,
        output_attentions: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        # Self attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states, attn_weights = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
        )
        hidden_states = residual + hidden_states

        # Feed forward
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states, attn_weights


class MoonshineStreamingDecoderLayer(nn.Module):
    """Decoder layer with flat structure matching Moonshine convention."""

    def __init__(self, config: MoonshineStreamingConfig, layer_idx: int):
        super().__init__()
        self.config = config

        self.self_attn = MoonshineStreamingAttention(
            config=config,
            layer_idx=layer_idx,
            is_causal=True,
            num_attention_heads=config.decoder_num_attention_heads,
            num_key_value_heads=config.decoder_num_attention_heads,
            dim=config.decoder_dim,
        )
        self.encoder_attn = MoonshineStreamingAttention(
            config=config,
            layer_idx=layer_idx,
            is_causal=False,
            num_attention_heads=config.decoder_num_attention_heads,
            num_key_value_heads=config.decoder_num_attention_heads,
            dim=config.decoder_dim,
        )
        self.mlp = MoonshineStreamingFeedForward(
            config.decoder_dim,
            mult=config.ffn_mult,
            use_swiglu=config.use_swiglu_decoder,
            dropout=config.ff_dropout,
        )
        self.input_layernorm = MoonshineStreamingLayerNorm(config.decoder_dim)
        self.post_attention_layernorm = MoonshineStreamingLayerNorm(config.decoder_dim)
        self.final_layernorm = MoonshineStreamingLayerNorm(config.decoder_dim)

    def forward(
        self,
        hidden_states: Tensor,
        encoder_hidden_states: Tensor,
        encoder_attention_mask: Optional[Tensor] = None,
        attention_mask: Optional[Tensor] = None,
        position_embeddings: Optional[tuple[Tensor, Tensor]] = None,
        output_attentions: bool = False,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
    ) -> tuple[Tensor, Optional[Tensor], Optional[Tensor]]:
        # Self attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            cache_position=cache_position,
        )
        hidden_states = residual + hidden_states

        # Cross attention with pre-norm on query only (matching original norm_q)
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states, cross_attn_weights = self.encoder_attn(
            hidden_states=hidden_states,
            key_value_states=encoder_hidden_states,
            attention_mask=encoder_attention_mask,
            past_key_values=past_key_values,
            cache_position=cache_position,
        )
        hidden_states = residual + hidden_states

        # Feed forward
        residual = hidden_states
        hidden_states = self.final_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states, self_attn_weights, cross_attn_weights


def make_frame_mask(lengths_samples: torch.Tensor, frame_len: int) -> tuple[torch.Tensor, torch.Tensor]:
    n_frames = lengths_samples // frame_len
    max_frames = n_frames.max()
    idx = torch.arange(max_frames, device=lengths_samples.device).unsqueeze(0)
    frame_mask = idx < n_frames.unsqueeze(1)
    return frame_mask, n_frames


def downsample_mask_causal(mask: torch.Tensor, kernel: int, stride: int, dilation: int = 1) -> torch.Tensor:
    m = mask.float().unsqueeze(1)
    left_pad = (kernel - 1) * dilation
    m_pad = F.pad(m, (left_pad, 0))
    weight = torch.ones(1, 1, kernel, device=mask.device)
    m_conv = F.conv1d(m_pad, weight, stride=stride, dilation=dilation)
    return (m_conv > 0).squeeze(1)


def frame_nonoverlap_drop_tail(
    wav: torch.Tensor, lengths: torch.Tensor, frame_len: int
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    batch_size, max_length = wav.shape
    lengths = lengths.clamp(max=max_length)
    frame_mask, n_frames = make_frame_mask(lengths, frame_len)
    max_frames = frame_mask.size(1)

    frames = wav.new_zeros(batch_size, max_frames, frame_len)
    for idx in range(batch_size):
        frame_count = int(n_frames[idx].item())
        if frame_count > 0:
            trunc = frame_count * frame_len
            frames[idx, :frame_count] = wav[idx, :trunc].reshape(frame_count, frame_len)
    return frames, frame_mask, n_frames


def bernoulli_replace_with_gaussian(
    values: torch.Tensor, valid_mask: torch.Tensor, p: float, sigma: float
) -> torch.Tensor:
    batch_size, seq_len, _ = values.shape
    flips = (torch.rand(batch_size, seq_len, device=values.device) < p) & valid_mask
    noise = torch.randn_like(values) * sigma
    replace_mask = flips.unsqueeze(-1)
    return torch.where(replace_mask, noise, values)


def make_sliding_window_mask(seq_len: int, n_past: int, n_future: int, device: torch.device) -> Tensor:
    q_idx = torch.arange(seq_len, device=device).unsqueeze(1)
    kv_idx = torch.arange(seq_len, device=device).unsqueeze(0)
    return (kv_idx >= q_idx - n_past) & (kv_idx <= q_idx + n_future)


class MoonshineStreamingEncoder(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.config = config
        self.dim = config.encoder_dim
        self.rotary_dim = config.encoder_rotary_dim

        # Audio preprocessing layers (previously in MoonshineStreamingAudioPreprocessor)
        self.frame_len = int(round(config.sample_rate * config.frame_ms / 1000.0))
        self.input_dropout_p = config.preprocessor_input_dropout_p
        self.input_dropout_sigma = config.preprocessor_input_dropout_sigma

        self.cmvn = MoonshineStreamingFrameCMVN()
        self.comp = MoonshineStreamingAsinhCompress(k_init=config.preprocessor_asinh_k_init)
        self.lin = nn.Linear(self.frame_len, config.encoder_dim, bias=False)
        self.act = nn.SiLU()

        self.conv1 = MoonshineStreamingCausalConv1d(
            config.encoder_dim,
            config.preprocessor_c1,
            kernel=config.preprocessor_k1,
            stride=2,
            bias=True,
        )
        self.conv2 = MoonshineStreamingCausalConv1d(
            config.preprocessor_c1,
            config.preprocessor_c2,
            kernel=config.preprocessor_k2,
            stride=2,
            bias=True,
        )

        self.k1 = config.preprocessor_k1
        self.k2 = config.preprocessor_k2

        # Encoder layers
        if config.encoder_window is None:
            self.windows = [None] * config.encoder_num_hidden_layers
        elif isinstance(config.encoder_window, list):
            if len(config.encoder_window) != config.encoder_num_hidden_layers:
                raise ValueError(
                    f"encoder_window length {len(config.encoder_window)} != depth {config.encoder_num_hidden_layers}"
                )
            self.windows = config.encoder_window
        else:
            self.windows = [config.encoder_window] * config.encoder_num_hidden_layers

        self.layers = nn.ModuleList(
            [MoonshineStreamingEncoderLayer(config, idx) for idx in range(config.encoder_num_hidden_layers)]
        )
        if self.rotary_dim > 0:
            self.rotary = MoonshineStreamingRotaryEmbedding(
                self.rotary_dim,
                base=config.rotary_base,
                interpolation_factor=config.rotary_interpolation_factor,
            )
        else:
            self.rotary = None

        self.final_norm = MoonshineStreamingLayerNorm(self.dim)

    def _preprocess(
        self, wav: torch.Tensor, lengths_samples: Optional[torch.Tensor] = None
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Preprocess raw audio waveform into features for the encoder."""
        batch_size, max_length = wav.shape
        if lengths_samples is None:
            lengths_samples = torch.full((batch_size,), max_length, dtype=torch.long, device=wav.device)

        frames, frame_mask, _ = frame_nonoverlap_drop_tail(wav, lengths_samples, self.frame_len)

        if frames.numel() == 0:
            channels_out = self.conv2.conv.weight.shape[0]
            empty = torch.zeros(batch_size, 0, channels_out, device=wav.device)
            empty_mask = torch.zeros(batch_size, 0, dtype=torch.bool, device=wav.device)
            empty_lengths = torch.zeros(batch_size, dtype=torch.long, device=wav.device)
            return empty, empty_mask, empty_lengths

        x = self.cmvn(frames)
        if self.training and self.input_dropout_p > 0:
            x = bernoulli_replace_with_gaussian(x, frame_mask, self.input_dropout_p, self.input_dropout_sigma)

        x = self.comp(x)
        # Cast to model dtype before linear layers (important for fp16/bf16 inference)
        x = x.to(dtype=self.lin.weight.dtype)
        x = self.lin(x)
        x = self.act(x)
        x = x * frame_mask.unsqueeze(-1)

        x = x.transpose(1, 2).contiguous()
        mask1 = downsample_mask_causal(frame_mask, kernel=self.k1, stride=2)
        x = self.conv1(x)
        x = self.act(x) * mask1.unsqueeze(1)

        mask2 = downsample_mask_causal(mask1, kernel=self.k2, stride=2)
        x = self.conv2(x)
        x = x * mask2.unsqueeze(1)

        feats = x.transpose(1, 2).contiguous()
        out_lengths = mask2.sum(dim=1)
        return feats, mask2, out_lengths

    def forward(
        self,
        input_values: Tensor,
        attention_mask: Optional[Tensor] = None,
        output_hidden_states: bool = False,
        output_attentions: bool = False,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[BaseModelOutput, tuple[Tensor, Optional[tuple[Tensor, ...]], Optional[tuple[Tensor, ...]]]]:
        del kwargs

        # Determine if input is raw audio (2D) or already preprocessed features (3D)
        if input_values.dim() == 2:
            # Raw audio input: (batch_size, audio_length)
            if input_values.dtype != torch.float32:
                input_values = input_values.to(dtype=torch.float32)

            if attention_mask is None:
                lengths = torch.full(
                    (input_values.shape[0],),
                    input_values.shape[-1],
                    dtype=torch.long,
                    device=input_values.device,
                )
            else:
                attention_mask = attention_mask.to(dtype=torch.long)
                lengths = attention_mask.sum(-1)
                seq_len_audio = attention_mask.shape[-1]
                expected = torch.arange(seq_len_audio, device=attention_mask.device).unsqueeze(0) < lengths.unsqueeze(
                    1
                )
                contiguous = torch.eq(attention_mask.bool(), expected).all(dim=1)
                lengths = torch.where(
                    contiguous,
                    lengths,
                    torch.full_like(lengths, seq_len_audio),
                )

            x, mask, _ = self._preprocess(input_values, lengths)
            # Cast to encoder dtype
            encoder_dtype = self.layers[0].self_attn.q_proj.weight.dtype
            if x.dtype != encoder_dtype:
                x = x.to(dtype=encoder_dtype)
        else:
            # Already preprocessed features: (batch_size, seq_len, encoder_dim)
            x = input_values
            mask = attention_mask

        _, seq_len, _ = x.shape

        hidden_states = (x,) if output_hidden_states else None
        all_attentions = () if output_attentions else None

        position_embeddings = None
        if self.rotary is not None:
            position_embeddings = self.rotary(seq_len, x.device, dtype=x.dtype)

        attn_implementation = getattr(self.config, "_attn_implementation", None) or "eager"
        if output_attentions and attn_implementation != "eager":
            logger.warning_once(
                "MoonshineStreaming attention does not support `output_attentions=True` with "
                f"`attn_implementation={attn_implementation}`. Falling back to eager attention."
            )
            attn_implementation = "eager"

        global_attn_mask = None
        if mask is not None:
            if "flash_attention" in attn_implementation:
                global_attn_mask = mask if (mask == 0).any() else None
            elif attn_implementation == "sdpa":
                global_attn_mask = _prepare_4d_attention_mask_for_sdpa(mask, x.dtype)
            else:
                global_attn_mask = _prepare_4d_attention_mask(mask, x.dtype)

        for layer, window in zip(self.layers, self.windows):
            attn_mask = global_attn_mask
            if window is not None:
                n_past, n_future = window
                window_mask = make_sliding_window_mask(seq_len, n_past, n_future, x.device)
                window_mask = window_mask.unsqueeze(0).unsqueeze(0)
                if attn_mask is not None and attn_mask.dim() == 2:
                    attn_mask = _prepare_4d_attention_mask(attn_mask, x.dtype)
                if attn_mask is None:
                    attn_mask = torch.zeros_like(window_mask, dtype=x.dtype)
                    attn_mask = attn_mask.masked_fill(window_mask.logical_not(), torch.finfo(x.dtype).min)
                else:
                    attn_mask = attn_mask.masked_fill(window_mask.logical_not(), torch.finfo(x.dtype).min)

            x, attn_weights = layer(
                x,
                position_embeddings=position_embeddings,
                attention_mask=attn_mask,
                output_attentions=output_attentions,
            )
            if output_hidden_states:
                hidden_states = hidden_states + (x,)
            if output_attentions:
                all_attentions = all_attentions + (attn_weights,)

        x = self.final_norm(x)
        outputs = BaseModelOutput(
            last_hidden_state=x,
            hidden_states=hidden_states,
            attentions=all_attentions,
        )
        if return_dict is False:
            return outputs.to_tuple()
        return outputs


class MoonshineStreamingContextAdapter(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.encoder_dim = config.encoder_dim
        self.decoder_dim = config.decoder_dim
        self.dropout_p = config.adapter_dropout
        self.block_size = config.adapter_block_size

        self.pos_embed = nn.Embedding(config.adapter_max_positions, config.encoder_dim)

        if config.encoder_dim != config.decoder_dim:
            self.proj = nn.Linear(config.encoder_dim, config.decoder_dim, bias=False)
        else:
            self.proj = nn.Identity()

    def forward(self, encoder_hidden: Tensor, valid_lengths: Optional[Tensor]) -> Tensor:
        batch_size, seq_len, _ = encoder_hidden.shape
        device = encoder_hidden.device

        if valid_lengths is None:
            valid_lengths = torch.full((batch_size,), seq_len, device=device, dtype=torch.long)

        time_ids = torch.arange(seq_len, device=device).unsqueeze(0)
        valid_mask = time_ids < valid_lengths.unsqueeze(1)

        pos_emb = self.pos_embed(time_ids.expand(batch_size, seq_len))
        x = encoder_hidden + pos_emb
        x = x * valid_mask.unsqueeze(-1)

        if self.training and self.dropout_p > 0.0:
            block_ids = (time_ids // self.block_size).expand(batch_size, seq_len)
            max_blocks = math.ceil(seq_len / self.block_size)
            block_keep = torch.rand(batch_size, max_blocks, device=device) > self.dropout_p
            time_mask = block_keep.gather(1, block_ids)
            time_mask = time_mask & valid_mask
            scale = 1.0 / (1.0 - self.dropout_p)
            x = x * time_mask.unsqueeze(-1) * scale

        return self.proj(x)


class MoonshineStreamingDecoder(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.config = config
        self.dim = config.decoder_dim
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.decoder_dim)
        nn.init.kaiming_normal_(self.embed_tokens.weight)

        self.layers = nn.ModuleList(
            [MoonshineStreamingDecoderLayer(config, idx) for idx in range(config.decoder_num_hidden_layers)]
        )
        if config.decoder_rotary_dim > 0:
            self.rotary = MoonshineStreamingRotaryEmbedding(
                config.decoder_rotary_dim,
                base=config.rotary_base,
                interpolation_factor=config.rotary_interpolation_factor,
            )
        else:
            self.rotary = None
        self.final_norm = MoonshineStreamingLayerNorm(config.decoder_dim)
        self.final_dropout = nn.Dropout(config.ff_dropout)

    def forward(
        self,
        input_ids: Optional[Tensor] = None,
        inputs_embeds: Optional[Tensor] = None,
        encoder_hidden_states: Optional[Tensor] = None,
        encoder_attention_mask: Optional[Tensor] = None,
        attention_mask: Optional[Tensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[Tensor] = None,
        output_hidden_states: bool = False,
        output_attentions: bool = False,
    ) -> BaseModelOutputWithPastAndCrossAttentions:
        if (input_ids is None) == (inputs_embeds is None):
            raise ValueError("Specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids.long())

        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if use_cache and past_key_values is None:
            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))

        batch_size, seq_len, _ = inputs_embeds.shape
        if encoder_hidden_states is None:
            raise ValueError("encoder_hidden_states must be provided")

        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0
        if cache_position is None:
            cache_position = torch.arange(
                past_key_values_length, past_key_values_length + seq_len, device=inputs_embeds.device
            )
        if attention_mask is not None and attention_mask.dim() == 2:
            expected_len = past_key_values_length + seq_len
            if attention_mask.shape[-1] != expected_len:
                if attention_mask.shape[-1] > expected_len:
                    attention_mask = attention_mask[:, -expected_len:]
                else:
                    attention_mask = F.pad(
                        attention_mask,
                        (expected_len - attention_mask.shape[-1], 0),
                        value=1,
                    )

        position_embeddings = None
        if self.rotary is not None:
            position_embeddings = self.rotary(
                seq_len, inputs_embeds.device, positions=cache_position, dtype=inputs_embeds.dtype
            )

        attn_implementation = getattr(self.config, "_attn_implementation", None) or "eager"
        if output_attentions and attn_implementation != "eager":
            logger.warning_once(
                "MoonshineStreaming attention does not support `output_attentions=True` with "
                f"`attn_implementation={attn_implementation}`. Falling back to eager attention."
            )
            attn_implementation = "eager"

        self_attn_mask = None
        if "flash_attention" in attn_implementation:
            if attention_mask is not None and (attention_mask == 0).any():
                self_attn_mask = attention_mask
        elif attn_implementation == "sdpa":
            self_attn_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                attention_mask,
                inputs_embeds.shape[:-1],
                inputs_embeds,
                past_key_values_length=past_key_values_length,
            )
        else:
            self_attn_mask = _prepare_4d_causal_attention_mask(
                attention_mask,
                inputs_embeds.shape[:-1],
                inputs_embeds,
                past_key_values_length=past_key_values_length,
            )

        cross_attn_mask = None
        if encoder_attention_mask is not None:
            if "flash_attention" in attn_implementation:
                cross_attn_mask = encoder_attention_mask if (encoder_attention_mask == 0).any() else None
            elif attn_implementation == "sdpa":
                cross_attn_mask = _prepare_4d_attention_mask_for_sdpa(
                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_len
                )
            else:
                cross_attn_mask = _prepare_4d_attention_mask(
                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_len
                )

        hidden_states = (inputs_embeds,) if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_cross_attns = () if output_attentions else None
        x = inputs_embeds
        for layer in self.layers:
            x, self_attn_weights, cross_attn_weights = layer(
                x,
                encoder_hidden_states,
                encoder_attention_mask=cross_attn_mask,
                attention_mask=self_attn_mask,
                position_embeddings=position_embeddings,
                output_attentions=output_attentions,
                past_key_values=past_key_values,
                cache_position=cache_position,
            )
            if output_hidden_states:
                hidden_states = hidden_states + (x,)
            if output_attentions:
                all_self_attns = all_self_attns + (self_attn_weights,)
                all_cross_attns = all_cross_attns + (cross_attn_weights,)

        x = self.final_norm(x)
        x = self.final_dropout(x)

        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=x,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=hidden_states,
            attentions=all_self_attns,
            cross_attentions=all_cross_attns,
        )


@auto_docstring
class MoonshineStreamingPreTrainedModel(PreTrainedModel):
    config: MoonshineStreamingConfig
    base_model_prefix = "model"
    main_input_name = "input_values"
    input_modalities = "audio"
    supports_gradient_checkpointing = False
    _no_split_modules = ["MoonshineStreamingEncoderLayer", "MoonshineStreamingDecoderLayer"]
    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = True
    config_class = MoonshineStreamingConfig
    # TODO arthur, how do we separate when it cross / self coming from different layer?

    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor) -> torch.LongTensor:
        """
        Computes the output length of the convolutional layers for MoonshineStreaming.
        Different from Moonshine due to frame-based preprocessing with causal convolutions.
        """
        frame_len = int(round(self.config.sample_rate * self.config.frame_ms / 1000.0))
        output_lengths = input_lengths // frame_len
        output_lengths = (output_lengths - 1) // 2 + 1
        output_lengths = (output_lengths - 1) // 2 + 1
        return output_lengths


class MoonshineStreamingModel(MoonshineStreamingPreTrainedModel):
    """
    The bare MoonshineStreaming encoder-decoder model outputting raw hidden-states without any specific head on top.

    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Args:
        config ([`MoonshineStreamingConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.

    Example:
        ```python
        >>> import torch
        >>> from transformers import MoonshineStreamingModel

        >>> model = MoonshineStreamingModel.from_pretrained("UsefulSensors/moonshine-streaming-tiny")

        >>> # Generate random audio input (batch_size=1, 16000 samples = 1 second of audio at 16kHz)
        >>> input_values = torch.randn(1, 16000)
        >>> decoder_input_ids = torch.tensor([[1]])  # Start token

        >>> outputs = model(input_values=input_values, decoder_input_ids=decoder_input_ids)
        >>> last_hidden_state = outputs.last_hidden_state
        ```
    """

    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__(config)
        self.encoder = MoonshineStreamingEncoder(config)
        self.adapter = MoonshineStreamingContextAdapter(config)
        self.decoder = MoonshineStreamingDecoder(config)
        self.post_init()

    def get_input_embeddings(self) -> nn.Module:
        return self.decoder.embed_tokens

    def set_input_embeddings(self, value: nn.Module):
        self.decoder.embed_tokens = value

    def forward(
        self,
        input_values: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[BaseModelOutput] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        decoder_inputs_embeds: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.Tensor] = None,
        use_cache: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Seq2SeqModelOutput:
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if encoder_outputs is None:
            if input_values is None:
                raise ValueError("input_values must be provided when encoder_outputs is None")
            # Pass raw audio directly to encoder (preprocessing is now handled internally)
            encoder_outputs = self.encoder(
                input_values,
                attention_mask=attention_mask,
                output_hidden_states=output_hidden_states,
                output_attentions=output_attentions,
            )
            # Get encoder attention mask from encoder's preprocessing
            if attention_mask is not None:
                lengths = attention_mask.sum(-1).to(dtype=torch.long)
                encoder_lengths = self._get_feat_extract_output_lengths(lengths)
                seq_len = encoder_outputs.last_hidden_state.shape[1]
                encoder_attention_mask = torch.arange(
                    seq_len, device=encoder_outputs.last_hidden_state.device
                ) < encoder_lengths.unsqueeze(1)
        else:
            if not isinstance(encoder_outputs, BaseModelOutput):
                encoder_outputs = BaseModelOutput(
                    last_hidden_state=encoder_outputs[0],
                    hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
                    attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
                )
            if encoder_attention_mask is None and attention_mask is not None:
                lengths = attention_mask.sum(-1).to(dtype=torch.long)
                encoder_lengths = self._get_feat_extract_output_lengths(lengths)
                seq_len = encoder_outputs.last_hidden_state.shape[1]
                encoder_attention_mask = torch.arange(
                    seq_len, device=encoder_outputs.last_hidden_state.device
                ) < encoder_lengths.unsqueeze(1)
            elif encoder_attention_mask is not None:
                encoder_attention_mask = encoder_attention_mask.to(dtype=torch.bool)

        encoder_hidden_states = encoder_outputs.last_hidden_state
        if encoder_attention_mask is not None:
            encoder_lengths = encoder_attention_mask.sum(-1)
        else:
            encoder_lengths = torch.full(
                (encoder_hidden_states.shape[0],),
                encoder_hidden_states.shape[1],
                dtype=torch.long,
                device=encoder_hidden_states.device,
            )

        adapted_states = self.adapter(encoder_hidden_states, encoder_lengths)

        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            inputs_embeds=decoder_inputs_embeds,
            encoder_hidden_states=adapted_states,
            encoder_attention_mask=encoder_attention_mask,
            attention_mask=decoder_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
        )

        if not return_dict:
            return Seq2SeqModelOutput(
                last_hidden_state=decoder_outputs.last_hidden_state,
                past_key_values=decoder_outputs.past_key_values,
                decoder_hidden_states=decoder_outputs.hidden_states,
                decoder_attentions=decoder_outputs.attentions,
                cross_attentions=decoder_outputs.cross_attentions,
                encoder_last_hidden_state=encoder_outputs.last_hidden_state,
                encoder_hidden_states=encoder_outputs.hidden_states,
                encoder_attentions=encoder_outputs.attentions,
            ).to_tuple()

        return Seq2SeqModelOutput(
            last_hidden_state=decoder_outputs.last_hidden_state,
            past_key_values=decoder_outputs.past_key_values,
            decoder_hidden_states=decoder_outputs.hidden_states,
            decoder_attentions=decoder_outputs.attentions,
            cross_attentions=decoder_outputs.cross_attentions,
            encoder_last_hidden_state=encoder_outputs.last_hidden_state,
            encoder_hidden_states=encoder_outputs.hidden_states,
            encoder_attentions=encoder_outputs.attentions,
        )


def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):
    """
    Shift input ids one token to the right.
    """
    shifted_input_ids = input_ids.new_zeros(input_ids.shape)
    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
    shifted_input_ids[:, 0] = decoder_start_token_id

    if pad_token_id is None:
        raise ValueError("self.model.config.pad_token_id has to be defined.")
    # replace possible -100 values in labels by `pad_token_id`
    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)

    return shifted_input_ids


@auto_docstring(
    custom_intro="""
    The MoonshineStreaming Model with a language modeling head. Can be used for automatic speech recognition.
    """
)
class MoonshineStreamingForConditionalGeneration(MoonshineStreamingPreTrainedModel, GenerationMixin):
    """
    The MoonshineStreaming model with a language modeling head for speech-to-text transcription.

    This model inherits from [`MoonshineForConditionalGeneration`]. Check the superclass documentation for the generic
    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Args:
        config ([`MoonshineStreamingConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.

    Example:
        ```python
        >>> import torch
        >>> from transformers import AutoProcessor, MoonshineStreamingForConditionalGeneration
        >>> from datasets import load_dataset

        >>> processor = AutoProcessor.from_pretrained("UsefulSensors/moonshine-streaming-tiny")
        >>> model = MoonshineStreamingForConditionalGeneration.from_pretrained("UsefulSensors/moonshine-streaming-tiny")

        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:1]")
        >>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")

        >>> generated_ids = model.generate(**inputs, max_new_tokens=100)
        >>> transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        >>> print(transcript)
        'Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'
        ```
    """

    _tied_weights_keys = {"proj_out.weight": "model.decoder.embed_tokens.weight"}

    config_class = MoonshineStreamingConfig
    supports_gradient_checkpointing = False

    def __init__(self, config: MoonshineStreamingConfig):
        # Call grandparent's __init__ to skip MoonshineForConditionalGeneration's model creation
        super().__init__(config)
        self.model = MoonshineStreamingModel(config)
        self.proj_out = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def get_output_embeddings(self):
        return self.proj_out

    def set_output_embeddings(self, new_embeddings):
        self.proj_out = new_embeddings

    def get_input_embeddings(self) -> nn.Module:
        return self.model.get_input_embeddings()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[BaseModelOutput] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        decoder_inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.Tensor] = None,
        use_cache: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Seq2SeqLMOutput:
        r"""
        input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):
            Float values of the raw speech waveform. Raw speech waveform can be
            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a
            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or
            the soundfile library (`pip install soundfile`). To prepare the array into
            `input_values`, the [`AutoFeatureExtractor`] should be used for padding
            and conversion into a tensor of type `torch.FloatTensor`.
        decoder_position_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`):
            Indices of positions of each input sequence tokens in the position embeddings.
            Used to calculate the position embeddings up to `config.decoder_config.max_position_embeddings`

        Example:

        ```python
        >>> import torch
        >>> from transformers import AutoProcessor, MoonshineStreamingForConditionalGeneration
        >>> from datasets import load_dataset

        >>> processor = AutoProcessor.from_pretrained("UsefulSensors/moonshine_streaming-tiny")
        >>> model = MoonshineStreamingForConditionalGeneration.from_pretrained("UsefulSensors/moonshine_streaming-tiny")

        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")

        >>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")
        >>> input_values = inputs.input_values

        >>> generated_ids = model.generate(input_values, max_new_tokens=100)

        >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        >>> transcription
        'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'
        ```"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
            decoder_input_ids = shift_tokens_right(
                labels, self.config.pad_token_id, self.config.decoder_start_token_id
            )
        if labels is not None:
            if use_cache:
                logger.warning("The `use_cache` argument is changed to `False` since `labels` is provided.")
            use_cache = False

        outputs = self.model(
            input_values=input_values,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            encoder_outputs=encoder_outputs,
            encoder_attention_mask=encoder_attention_mask,
            decoder_inputs_embeds=decoder_inputs_embeds,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
            return_dict=return_dict,
        )

        if return_dict:
            decoder_hidden = outputs.last_hidden_state
        else:
            decoder_hidden = outputs[0]

        logits = self.proj_out(decoder_hidden)
        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return Seq2SeqLMOutput(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            decoder_hidden_states=outputs.decoder_hidden_states,
            decoder_attentions=outputs.decoder_attentions,
            cross_attentions=outputs.cross_attentions,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_hidden_states=outputs.encoder_hidden_states,
            encoder_attentions=outputs.encoder_attentions,
        )

    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor) -> torch.LongTensor:
        """
        Computes the output length of the convolutional layers for MoonshineStreaming.
        Different from Moonshine due to frame-based preprocessing with causal convolutions.
        """
        frame_len = int(round(self.config.sample_rate * self.config.frame_ms / 1000.0))
        output_lengths = input_lengths // frame_len
        output_lengths = (output_lengths - 1) // 2 + 1
        output_lengths = (output_lengths - 1) // 2 + 1
        return output_lengths

    def _prepare_encoder_decoder_kwargs_for_generation(
        self,
        inputs_tensor: torch.Tensor,
        model_kwargs,
        model_input_name: Optional[str],
        generation_config,
    ):
        del model_input_name
        attention_mask = model_kwargs.get("attention_mask", None)

        # Pass raw audio directly to encoder (preprocessing is now handled internally)
        encoder_outputs = self.model.encoder(
            input_values=inputs_tensor,
            attention_mask=attention_mask,
            output_attentions=generation_config.output_attentions,
            output_hidden_states=generation_config.output_hidden_states,
            return_dict=True,
        )

        # Compute encoder attention mask from input lengths
        if attention_mask is not None:
            lengths = attention_mask.sum(-1).to(dtype=torch.long)
            encoder_lengths = self._get_feat_extract_output_lengths(lengths)
            seq_len = encoder_outputs.last_hidden_state.shape[1]
            encoder_attention_mask = torch.arange(
                seq_len, device=encoder_outputs.last_hidden_state.device
            ) < encoder_lengths.unsqueeze(1)
        else:
            encoder_attention_mask = None

        model_kwargs["encoder_outputs"] = encoder_outputs
        model_kwargs["encoder_attention_mask"] = encoder_attention_mask
        model_kwargs["attention_mask"] = encoder_attention_mask
        return model_kwargs


__all__ = [
    "MoonshineStreamingModel",
    "MoonshineStreamingPreTrainedModel",
    "MoonshineStreamingForConditionalGeneration",
]
