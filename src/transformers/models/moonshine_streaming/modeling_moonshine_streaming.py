#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/moonshine_streaming/modular_moonshine_streaming.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_moonshine_streaming.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2026 the HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from typing import Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache
from ...generation import GenerationMixin
from ...modeling_attn_mask_utils import (
    _prepare_4d_attention_mask,
    _prepare_4d_attention_mask_for_sdpa,
    _prepare_4d_causal_attention_mask,
    _prepare_4d_causal_attention_mask_for_sdpa,
)
from ...modeling_outputs import (
    BaseModelOutput,
    BaseModelOutputWithPastAndCrossAttentions,
    Seq2SeqLMOutput,
    Seq2SeqModelOutput,
)
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...utils import logging
from .configuration_moonshine_streaming import MoonshineStreamingConfig


logger = logging.get_logger(__name__)


class MoonshineStreamingFrameCMVN(nn.Module):
    def __init__(self, eps: float = 1e-6):
        super().__init__()
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        mean = x.mean(dim=-1, keepdim=True)
        centered = x - mean
        rms = (centered.pow(2).mean(dim=-1, keepdim=True) + self.eps).sqrt()
        return centered / rms


class MoonshineStreamingAsinhCompress(nn.Module):
    def __init__(self, k_init: float = 0.75):
        super().__init__()
        self.log_k = nn.Parameter(torch.log(torch.tensor(k_init)))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.asinh(torch.exp(self.log_k) * x)


class MoonshineStreamingCausalConv1d(nn.Module):
    def __init__(
        self,
        c_in: int,
        c_out: int,
        kernel: int,
        stride: int = 1,
        dilation: int = 1,
        bias: bool = True,
    ):
        super().__init__()
        self.left_pad = (kernel - 1) * dilation
        self.conv = nn.utils.parametrizations.weight_norm(
            nn.Conv1d(c_in, c_out, kernel, stride=stride, dilation=dilation, bias=bias)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = F.pad(x, (self.left_pad, 0))
        return self.conv(x)


class MoonshineStreamingRotaryEmbedding(nn.Module):
    def __init__(
        self,
        dim: int,
        base: float = 10000.0,
        interpolation_factor: float = 1.0,
        device: Optional[str] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        super().__init__()
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device, dtype=dtype or torch.float32) / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.interpolation_factor = interpolation_factor

    def forward(self, seq_len: Optional[int], device: torch.device, positions: Optional[Tensor] = None) -> Tensor:
        if positions is None:
            if seq_len is None:
                raise ValueError("seq_len must be provided when positions is None.")
            positions = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)
        else:
            positions = positions.to(device=device, dtype=self.inv_freq.dtype).view(-1)

        freqs = torch.einsum("i,j->ij", positions, self.inv_freq.to(device)) / self.interpolation_factor
        return torch.stack((freqs, freqs), dim=-1).flatten(-2)


class MoonshineStreamingLayerNorm(nn.Module):
    def __init__(self, dim: int, unit_offset: bool = True, device=None, dtype=None):
        super().__init__()
        self.unit_offset = unit_offset
        self.ln = nn.LayerNorm(dim, elementwise_affine=False, device=device, dtype=dtype)
        self.gamma = nn.Parameter(torch.ones(dim, device=device, dtype=dtype))
        nn.init.constant_(self.gamma, 1.0 - float(unit_offset))

    def forward(self, x: Tensor) -> Tensor:
        normed = self.ln(x)
        gamma = self.gamma + float(self.unit_offset)
        return normed * gamma


class MoonshineStreamingSwiGLU(nn.Module):
    def __init__(self, dim_in: int, dim_out: int, device=None, dtype=None):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2, device=device, dtype=dtype)

    def forward(self, x: Tensor) -> Tensor:
        x, gate = self.proj(x).chunk(2, dim=-1)
        return x * F.silu(gate)


class MoonshineStreamingFeedForward(nn.Module):
    def __init__(
        self,
        dim: int,
        mult: int = 4,
        use_swiglu: bool = True,
        dropout: float = 0.1,
        device=None,
        dtype=None,
    ):
        super().__init__()
        inner_dim = dim * mult
        if use_swiglu:
            self.project_in = MoonshineStreamingSwiGLU(dim, inner_dim, device=device, dtype=dtype)
        else:
            self.project_in = nn.Sequential(nn.Linear(dim, inner_dim, device=device, dtype=dtype), nn.GELU())

        self.dropout = nn.Dropout(dropout)
        self.project_out = nn.Linear(inner_dim, dim, bias=True, device=device, dtype=dtype)

    def forward(self, x: Tensor) -> Tensor:
        x = self.project_in(x)
        x = self.dropout(x)
        return self.project_out(x)


def eager_attention_forward(
    module: nn.Module,
    query: Tensor,
    key: Tensor,
    value: Tensor,
    attention_mask: Optional[Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    is_causal: Optional[bool] = None,
    **_kwargs,
) -> tuple[Tensor, Tensor]:
    if scaling is None:
        scaling = module.scaling

    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling

    if attention_mask is not None:
        if attention_mask.dim() == 2:
            attention_mask = _prepare_4d_attention_mask(
                attention_mask, dtype=attn_weights.dtype, tgt_len=attn_weights.shape[-2]
            )
        elif attention_mask.dim() == 3:
            attention_mask = attention_mask.unsqueeze(1)

        if attention_mask.dtype == torch.bool:
            attn_weights = attn_weights.masked_fill(~attention_mask, torch.finfo(attn_weights.dtype).min)
        else:
            attn_weights = attn_weights + attention_mask
    elif is_causal:
        q_len = attn_weights.shape[-2]
        k_len = attn_weights.shape[-1]
        causal_mask = torch.full((q_len, k_len), torch.finfo(attn_weights.dtype).min, device=attn_weights.device)
        causal_mask = torch.triu(causal_mask, diagonal=1)
        attn_weights = attn_weights + causal_mask

    attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = F.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


def rotate_half(x: Tensor) -> Tensor:
    x1, x2 = x.unflatten(-1, (-1, 2)).unbind(-1)
    return torch.stack((-x2, x1), dim=-1).flatten(-2)


def apply_rotary_pos_emb(x: Tensor, freqs: Tensor) -> Tensor:
    rot_dim = freqs.shape[-1]
    x_rot, x_pass = x[..., :rot_dim], x[..., rot_dim:]
    freqs = freqs.unsqueeze(0).unsqueeze(0)
    cos = freqs.cos().to(x.dtype)
    sin = freqs.sin().to(x.dtype)
    x_rot = x_rot * cos + rotate_half(x_rot) * sin
    return torch.cat([x_rot, x_pass], dim=-1)


class MoonshineStreamingMultiHeadAttention(nn.Module):
    def __init__(
        self,
        config: MoonshineStreamingConfig,
        dim: int,
        head_dim: int,
        nheads: int,
        dropout: float = 0.0,
        bias: bool = False,
        layer_idx: Optional[int] = None,
        device=None,
        dtype=None,
    ):
        super().__init__()
        self.config = config
        self.nheads = nheads
        self.head_dim = head_dim
        self.scaling = head_dim**-0.5
        self.attention_dropout = dropout
        self.layer_idx = layer_idx

        embed_dim = head_dim * nheads
        self.q_proj = nn.Linear(dim, embed_dim, bias=bias, device=device, dtype=dtype)
        self.k_proj = nn.Linear(dim, embed_dim, bias=bias, device=device, dtype=dtype)
        self.v_proj = nn.Linear(dim, embed_dim, bias=bias, device=device, dtype=dtype)
        self.out_proj = nn.Linear(embed_dim, dim, bias=bias, device=device, dtype=dtype)

    def forward(
        self,
        q: Tensor,
        k: Tensor,
        v: Tensor,
        rotary_freqs: Optional[Tensor] = None,
        attn_mask: Optional[Tensor] = None,
        is_causal: bool = False,
        output_attentions: bool = False,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
        is_cross_attention: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        batch_size, q_len, _ = q.shape

        query = self.q_proj(q).unflatten(-1, (self.nheads, self.head_dim)).transpose(1, 2)
        if past_key_values is not None and self.layer_idx is None:
            raise ValueError("layer_idx must be set when using past_key_values.")

        is_updated = False
        if past_key_values is not None:
            if isinstance(past_key_values, EncoderDecoderCache):
                is_updated = past_key_values.is_updated.get(self.layer_idx)
                if is_cross_attention:
                    current_past_key_values = past_key_values.cross_attention_cache
                else:
                    current_past_key_values = past_key_values.self_attention_cache
            else:
                current_past_key_values = past_key_values
        else:
            current_past_key_values = None

        if is_cross_attention and current_past_key_values is not None and is_updated:
            key = current_past_key_values.layers[self.layer_idx].keys
            value = current_past_key_values.layers[self.layer_idx].values
        else:
            key = self.k_proj(k).unflatten(-1, (self.nheads, self.head_dim)).transpose(1, 2)
            value = self.v_proj(v).unflatten(-1, (self.nheads, self.head_dim)).transpose(1, 2)

            if rotary_freqs is not None and not is_cross_attention:
                k_len = key.shape[2]
                key = apply_rotary_pos_emb(key, rotary_freqs[:k_len])

            if current_past_key_values is not None:
                cache_position = cache_position if not is_cross_attention else None
                key, value = current_past_key_values.update(
                    key, value, self.layer_idx, {"cache_position": cache_position}
                )
                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):
                    past_key_values.is_updated[self.layer_idx] = True

        if rotary_freqs is not None:
            query = apply_rotary_pos_emb(query, rotary_freqs[:q_len])

        attn_implementation = getattr(self.config, "_attn_implementation", None) or "eager"
        if output_attentions and attn_implementation != "eager":
            logger.warning_once(
                "MoonshineStreaming attention does not support `output_attentions=True` with "
                f"`attn_implementation={attn_implementation}`. Falling back to eager attention."
            )
            attn_implementation = "eager"

        attention_interface = eager_attention_forward
        if attn_implementation != "eager":
            if attn_mask is not None and attn_mask.dim() > 2 and "flash_attention" in attn_implementation:
                logger.warning_once(
                    "Flash attention does not support 4D attention masks. Falling back to SDPA for MoonshineStreaming."
                )
                attn_implementation = "sdpa"
            attention_interface = ALL_ATTENTION_FUNCTIONS[attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query,
            key,
            value,
            attn_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            is_causal=is_causal,
            output_attentions=output_attentions,
        )

        attn_output = attn_output.reshape(batch_size, q_len, -1).contiguous()
        attn_output = self.out_proj(attn_output)
        return attn_output, attn_weights


class MoonshineStreamingSelfAttentionBlock(nn.Module):
    def __init__(
        self,
        config: MoonshineStreamingConfig,
        dim: int,
        head_dim: int,
        nheads: int,
        use_swiglu: bool = True,
        ff_mult: int = 4,
        attn_dropout: float = 0.0,
        ff_dropout: float = 0.1,
        has_ff: bool = True,
        layer_idx: Optional[int] = None,
        device=None,
        dtype=None,
    ):
        super().__init__()
        self.norm1 = MoonshineStreamingLayerNorm(dim, device=device, dtype=dtype)
        self.attn = MoonshineStreamingMultiHeadAttention(
            config,
            dim,
            head_dim,
            nheads,
            dropout=attn_dropout,
            bias=config.attention_bias,
            layer_idx=layer_idx,
            device=device,
            dtype=dtype,
        )

        self.has_ff = has_ff
        if has_ff:
            self.norm2 = MoonshineStreamingLayerNorm(dim, device=device, dtype=dtype)
            self.ff = MoonshineStreamingFeedForward(
                dim,
                mult=ff_mult,
                use_swiglu=use_swiglu,
                dropout=ff_dropout,
                device=device,
                dtype=dtype,
            )

    def forward(
        self,
        x: Tensor,
        rotary_freqs: Optional[Tensor] = None,
        attn_mask: Optional[Tensor] = None,
        is_causal: bool = False,
        output_attentions: bool = False,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
    ) -> tuple[Tensor, Optional[Tensor]]:
        normed = self.norm1(x)
        attn_output, attn_weights = self.attn(
            normed,
            normed,
            normed,
            rotary_freqs,
            attn_mask,
            is_causal,
            output_attentions=output_attentions,
            past_key_values=past_key_values,
            cache_position=cache_position,
        )
        x = x + attn_output
        if self.has_ff:
            x = x + self.ff(self.norm2(x))
        return x, attn_weights


class MoonshineStreamingCrossAttentionBlock(nn.Module):
    def __init__(
        self,
        config: MoonshineStreamingConfig,
        dim: int,
        head_dim: int,
        nheads: int,
        use_swiglu: bool = True,
        ff_mult: int = 4,
        attn_dropout: float = 0.0,
        ff_dropout: float = 0.1,
        layer_idx: Optional[int] = None,
        device=None,
        dtype=None,
    ):
        super().__init__()
        self.norm_q = MoonshineStreamingLayerNorm(dim, device=device, dtype=dtype)
        self.attn = MoonshineStreamingMultiHeadAttention(
            config,
            dim,
            head_dim,
            nheads,
            dropout=attn_dropout,
            bias=config.attention_bias,
            layer_idx=layer_idx,
            device=device,
            dtype=dtype,
        )
        self.norm2 = MoonshineStreamingLayerNorm(dim, device=device, dtype=dtype)
        self.ff = MoonshineStreamingFeedForward(
            dim,
            mult=ff_mult,
            use_swiglu=use_swiglu,
            dropout=ff_dropout,
            device=device,
            dtype=dtype,
        )

    def forward(
        self,
        x: Tensor,
        context: Tensor,
        cross_attn_mask: Optional[Tensor] = None,
        output_attentions: bool = False,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
    ) -> tuple[Tensor, Optional[Tensor]]:
        normed_q = self.norm_q(x)
        attn_output, attn_weights = self.attn(
            normed_q,
            context,
            context,
            attn_mask=cross_attn_mask,
            output_attentions=output_attentions,
            past_key_values=past_key_values,
            cache_position=cache_position,
            is_cross_attention=True,
        )
        x = x + attn_output
        x = x + self.ff(self.norm2(x))
        return x, attn_weights


class MoonshineStreamingEncoderLayer(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig, layer_idx: int):
        super().__init__()
        self.block = MoonshineStreamingSelfAttentionBlock(
            config,
            dim=config.encoder_dim,
            head_dim=config.head_dim,
            nheads=config.encoder_num_attention_heads,
            use_swiglu=config.use_swiglu_encoder,
            ff_mult=config.ffn_mult,
            attn_dropout=config.attn_dropout,
            ff_dropout=config.ff_dropout,
            has_ff=True,
            layer_idx=layer_idx,
        )

    def forward(
        self,
        x: Tensor,
        rotary_freqs: Optional[Tensor] = None,
        attn_mask: Optional[Tensor] = None,
        output_attentions: bool = False,
    ) -> tuple[Tensor, Optional[Tensor]]:
        return self.block(
            x,
            rotary_freqs=rotary_freqs,
            attn_mask=attn_mask,
            output_attentions=output_attentions,
        )


def make_frame_mask(lengths_samples: torch.Tensor, frame_len: int) -> tuple[torch.Tensor, torch.Tensor]:
    n_frames = lengths_samples // frame_len
    max_frames = n_frames.max()
    idx = torch.arange(max_frames, device=lengths_samples.device).unsqueeze(0)
    frame_mask = idx < n_frames.unsqueeze(1)
    return frame_mask, n_frames


def downsample_mask_causal(mask: torch.Tensor, kernel: int, stride: int, dilation: int = 1) -> torch.Tensor:
    m = mask.float().unsqueeze(1)
    left_pad = (kernel - 1) * dilation
    m_pad = F.pad(m, (left_pad, 0))
    weight = torch.ones(1, 1, kernel, device=mask.device)
    m_conv = F.conv1d(m_pad, weight, stride=stride, dilation=dilation)
    return (m_conv > 0).squeeze(1)


def frame_nonoverlap_drop_tail(
    wav: torch.Tensor, lengths: torch.Tensor, frame_len: int
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    batch_size, max_length = wav.shape
    lengths = lengths.clamp(max=max_length)
    frame_mask, n_frames = make_frame_mask(lengths, frame_len)
    max_frames = frame_mask.size(1)

    frames = wav.new_zeros(batch_size, max_frames, frame_len)
    for idx in range(batch_size):
        frame_count = int(n_frames[idx].item())
        if frame_count > 0:
            trunc = frame_count * frame_len
            frames[idx, :frame_count] = wav[idx, :trunc].reshape(frame_count, frame_len)
    return frames, frame_mask, n_frames


def bernoulli_replace_with_gaussian(
    values: torch.Tensor, valid_mask: torch.Tensor, p: float, sigma: float
) -> torch.Tensor:
    batch_size, seq_len, _ = values.shape
    flips = (torch.rand(batch_size, seq_len, device=values.device) < p) & valid_mask
    noise = torch.randn_like(values) * sigma
    replace_mask = flips.unsqueeze(-1)
    return torch.where(replace_mask, noise, values)


def make_sliding_window_mask(seq_len: int, n_past: int, n_future: int, device: torch.device) -> Tensor:
    q_idx = torch.arange(seq_len, device=device).unsqueeze(1)
    kv_idx = torch.arange(seq_len, device=device).unsqueeze(0)
    return (kv_idx >= q_idx - n_past) & (kv_idx <= q_idx + n_future)


class MoonshineStreamingEncoder(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.config = config
        self.dim = config.encoder_dim
        self.rotary_dim = config.encoder_rotary_dim

        # Audio preprocessing layers (previously in MoonshineStreamingAudioPreprocessor)
        self.frame_len = int(round(config.sample_rate * config.frame_ms / 1000.0))
        self.input_dropout_p = config.preprocessor_input_dropout_p
        self.input_dropout_sigma = config.preprocessor_input_dropout_sigma

        self.cmvn = MoonshineStreamingFrameCMVN()
        self.comp = MoonshineStreamingAsinhCompress(k_init=config.preprocessor_asinh_k_init)
        self.lin = nn.Linear(self.frame_len, config.encoder_dim, bias=False)
        self.act = nn.SiLU()

        self.conv1 = MoonshineStreamingCausalConv1d(
            config.encoder_dim,
            config.preprocessor_c1,
            kernel=config.preprocessor_k1,
            stride=2,
            bias=True,
        )
        self.conv2 = MoonshineStreamingCausalConv1d(
            config.preprocessor_c1,
            config.preprocessor_c2,
            kernel=config.preprocessor_k2,
            stride=2,
            bias=True,
        )

        self.k1 = config.preprocessor_k1
        self.k2 = config.preprocessor_k2

        # Encoder layers
        if config.encoder_window is None:
            self.windows = [None] * config.encoder_num_hidden_layers
        elif isinstance(config.encoder_window, list):
            if len(config.encoder_window) != config.encoder_num_hidden_layers:
                raise ValueError(
                    f"encoder_window length {len(config.encoder_window)} != depth {config.encoder_num_hidden_layers}"
                )
            self.windows = config.encoder_window
        else:
            self.windows = [config.encoder_window] * config.encoder_num_hidden_layers

        self.layers = nn.ModuleList(
            [MoonshineStreamingEncoderLayer(config, idx) for idx in range(config.encoder_num_hidden_layers)]
        )
        if self.rotary_dim > 0:
            self.rotary = MoonshineStreamingRotaryEmbedding(
                self.rotary_dim,
                base=config.rotary_base,
                interpolation_factor=config.rotary_interpolation_factor,
            )
        else:
            self.rotary = None

        self.final_norm = MoonshineStreamingLayerNorm(self.dim)

    def _preprocess(
        self, wav: torch.Tensor, lengths_samples: Optional[torch.Tensor] = None
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Preprocess raw audio waveform into features for the encoder."""
        batch_size, max_length = wav.shape
        if lengths_samples is None:
            lengths_samples = torch.full((batch_size,), max_length, dtype=torch.long, device=wav.device)

        frames, frame_mask, _ = frame_nonoverlap_drop_tail(wav, lengths_samples, self.frame_len)

        if frames.numel() == 0:
            channels_out = self.conv2.conv.weight.shape[0]
            empty = torch.zeros(batch_size, 0, channels_out, device=wav.device)
            empty_mask = torch.zeros(batch_size, 0, dtype=torch.bool, device=wav.device)
            empty_lengths = torch.zeros(batch_size, dtype=torch.long, device=wav.device)
            return empty, empty_mask, empty_lengths

        x = self.cmvn(frames)
        if self.training and self.input_dropout_p > 0:
            x = bernoulli_replace_with_gaussian(x, frame_mask, self.input_dropout_p, self.input_dropout_sigma)

        x = self.comp(x)
        # Cast to model dtype before linear layers (important for fp16/bf16 inference)
        x = x.to(dtype=self.lin.weight.dtype)
        x = self.lin(x)
        x = self.act(x)
        x = x * frame_mask.unsqueeze(-1)

        x = x.transpose(1, 2).contiguous()
        mask1 = downsample_mask_causal(frame_mask, kernel=self.k1, stride=2)
        x = self.conv1(x)
        x = self.act(x) * mask1.unsqueeze(1)

        mask2 = downsample_mask_causal(mask1, kernel=self.k2, stride=2)
        x = self.conv2(x)
        x = x * mask2.unsqueeze(1)

        feats = x.transpose(1, 2).contiguous()
        out_lengths = mask2.sum(dim=1)
        return feats, mask2, out_lengths

    def forward(
        self,
        input_values: Tensor,
        attention_mask: Optional[Tensor] = None,
        output_hidden_states: bool = False,
        output_attentions: bool = False,
        return_dict: Optional[bool] = None,
        **kwargs,
    ) -> Union[BaseModelOutput, tuple[Tensor, Optional[tuple[Tensor, ...]], Optional[tuple[Tensor, ...]]]]:
        del kwargs

        # Determine if input is raw audio (2D) or already preprocessed features (3D)
        if input_values.dim() == 2:
            # Raw audio input: (batch_size, audio_length)
            if input_values.dtype != torch.float32:
                input_values = input_values.to(dtype=torch.float32)

            if attention_mask is None:
                lengths = torch.full(
                    (input_values.shape[0],),
                    input_values.shape[-1],
                    dtype=torch.long,
                    device=input_values.device,
                )
            else:
                attention_mask = attention_mask.to(dtype=torch.long)
                lengths = attention_mask.sum(-1)
                seq_len_audio = attention_mask.shape[-1]
                expected = torch.arange(seq_len_audio, device=attention_mask.device).unsqueeze(0) < lengths.unsqueeze(
                    1
                )
                contiguous = torch.eq(attention_mask.bool(), expected).all(dim=1)
                lengths = torch.where(
                    contiguous,
                    lengths,
                    torch.full_like(lengths, seq_len_audio),
                )

            x, mask, _ = self._preprocess(input_values, lengths)
            # Cast to encoder dtype
            encoder_dtype = self.layers[0].block.attn.q_proj.weight.dtype
            if x.dtype != encoder_dtype:
                x = x.to(dtype=encoder_dtype)
        else:
            # Already preprocessed features: (batch_size, seq_len, encoder_dim)
            x = input_values
            mask = attention_mask

        _, seq_len, _ = x.shape

        hidden_states = (x,) if output_hidden_states else None
        all_attentions = () if output_attentions else None

        rotary_freqs = None
        if self.rotary is not None:
            rotary_freqs = self.rotary(seq_len, x.device)

        attn_implementation = getattr(self.config, "_attn_implementation", None) or "eager"
        if output_attentions and attn_implementation != "eager":
            logger.warning_once(
                "MoonshineStreaming attention does not support `output_attentions=True` with "
                f"`attn_implementation={attn_implementation}`. Falling back to eager attention."
            )
            attn_implementation = "eager"

        global_attn_mask = None
        if mask is not None:
            if "flash_attention" in attn_implementation:
                global_attn_mask = mask if (mask == 0).any() else None
            elif attn_implementation == "sdpa":
                global_attn_mask = _prepare_4d_attention_mask_for_sdpa(mask, x.dtype)
            else:
                global_attn_mask = _prepare_4d_attention_mask(mask, x.dtype)

        for layer, window in zip(self.layers, self.windows):
            attn_mask = global_attn_mask
            if window is not None:
                n_past, n_future = window
                window_mask = make_sliding_window_mask(seq_len, n_past, n_future, x.device)
                window_mask = window_mask.unsqueeze(0).unsqueeze(0)
                if attn_mask is not None and attn_mask.dim() == 2:
                    attn_mask = _prepare_4d_attention_mask(attn_mask, x.dtype)
                if attn_mask is None:
                    attn_mask = torch.zeros_like(window_mask, dtype=x.dtype)
                    attn_mask = attn_mask.masked_fill(window_mask.logical_not(), torch.finfo(x.dtype).min)
                else:
                    attn_mask = attn_mask.masked_fill(window_mask.logical_not(), torch.finfo(x.dtype).min)

            x, attn_weights = layer(
                x,
                rotary_freqs=rotary_freqs,
                attn_mask=attn_mask,
                output_attentions=output_attentions,
            )
            if output_hidden_states:
                hidden_states = hidden_states + (x,)
            if output_attentions:
                all_attentions = all_attentions + (attn_weights,)

        x = self.final_norm(x)
        outputs = BaseModelOutput(
            last_hidden_state=x,
            hidden_states=hidden_states,
            attentions=all_attentions,
        )
        if return_dict is False:
            return outputs.to_tuple()
        return outputs


class MoonshineStreamingContextAdapter(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.encoder_dim = config.encoder_dim
        self.decoder_dim = config.decoder_dim
        self.dropout_p = config.adapter_dropout
        self.block_size = config.adapter_block_size

        self.pos_embed = nn.Embedding(config.adapter_max_positions, config.encoder_dim)

        if config.encoder_dim != config.decoder_dim:
            self.proj = nn.Linear(config.encoder_dim, config.decoder_dim, bias=False)
        else:
            self.proj = nn.Identity()

    def forward(self, encoder_hidden: Tensor, valid_lengths: Optional[Tensor]) -> Tensor:
        batch_size, seq_len, _ = encoder_hidden.shape
        device = encoder_hidden.device

        if valid_lengths is None:
            valid_lengths = torch.full((batch_size,), seq_len, device=device, dtype=torch.long)

        time_ids = torch.arange(seq_len, device=device).unsqueeze(0)
        valid_mask = time_ids < valid_lengths.unsqueeze(1)

        pos_emb = self.pos_embed(time_ids.expand(batch_size, seq_len))
        x = encoder_hidden + pos_emb
        x = x * valid_mask.unsqueeze(-1)

        if self.training and self.dropout_p > 0.0:
            block_ids = (time_ids // self.block_size).expand(batch_size, seq_len)
            max_blocks = math.ceil(seq_len / self.block_size)
            block_keep = torch.rand(batch_size, max_blocks, device=device) > self.dropout_p
            time_mask = block_keep.gather(1, block_ids)
            time_mask = time_mask & valid_mask
            scale = 1.0 / (1.0 - self.dropout_p)
            x = x * time_mask.unsqueeze(-1) * scale

        return self.proj(x)


class MoonshineStreamingDecoderLayer(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig, layer_idx: int):
        super().__init__()
        self.self_attn = MoonshineStreamingSelfAttentionBlock(
            config,
            dim=config.decoder_dim,
            head_dim=config.head_dim,
            nheads=config.decoder_num_attention_heads,
            use_swiglu=config.use_swiglu_decoder,
            ff_mult=config.ffn_mult,
            attn_dropout=config.attn_dropout,
            ff_dropout=config.ff_dropout,
            has_ff=False,
            layer_idx=layer_idx,
        )
        self.cross_attn = MoonshineStreamingCrossAttentionBlock(
            config,
            dim=config.decoder_dim,
            head_dim=config.head_dim,
            nheads=config.decoder_num_attention_heads,
            use_swiglu=config.use_swiglu_decoder,
            ff_mult=config.ffn_mult,
            attn_dropout=config.attn_dropout,
            ff_dropout=config.ff_dropout,
            layer_idx=layer_idx,
        )

    def forward(
        self,
        x: Tensor,
        context: Tensor,
        cross_attn_mask: Optional[Tensor] = None,
        rotary_freqs: Optional[Tensor] = None,
        self_attn_mask: Optional[Tensor] = None,
        output_attentions: bool = False,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[Tensor] = None,
    ) -> tuple[Tensor, Optional[Tensor], Optional[Tensor]]:
        x, self_attn_weights = self.self_attn(
            x,
            rotary_freqs=rotary_freqs,
            attn_mask=self_attn_mask,
            is_causal=True,
            output_attentions=output_attentions,
            past_key_values=past_key_values,
            cache_position=cache_position,
        )
        x, cross_attn_weights = self.cross_attn(
            x,
            context,
            cross_attn_mask=cross_attn_mask,
            output_attentions=output_attentions,
            past_key_values=past_key_values,
            cache_position=cache_position,
        )
        return x, self_attn_weights, cross_attn_weights


class MoonshineStreamingDecoder(nn.Module):
    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__()
        self.config = config
        self.dim = config.decoder_dim
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.decoder_dim)
        nn.init.kaiming_normal_(self.embed_tokens.weight)

        self.layers = nn.ModuleList(
            [MoonshineStreamingDecoderLayer(config, idx) for idx in range(config.decoder_num_hidden_layers)]
        )
        if config.decoder_rotary_dim > 0:
            self.rotary = MoonshineStreamingRotaryEmbedding(
                config.decoder_rotary_dim,
                base=config.rotary_base,
                interpolation_factor=config.rotary_interpolation_factor,
            )
        else:
            self.rotary = None
        self.final_norm = MoonshineStreamingLayerNorm(config.decoder_dim)
        self.final_dropout = nn.Dropout(config.ff_dropout)

    def get_logits(self, x: Tensor) -> Tensor:
        return x @ self.embed_tokens.weight.t()

    def forward(
        self,
        input_ids: Optional[Tensor] = None,
        inputs_embeds: Optional[Tensor] = None,
        encoder_hidden_states: Optional[Tensor] = None,
        encoder_attention_mask: Optional[Tensor] = None,
        decoder_attention_mask: Optional[Tensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[Tensor] = None,
        output_hidden_states: bool = False,
        output_attentions: bool = False,
    ) -> BaseModelOutputWithPastAndCrossAttentions:
        if (input_ids is None) == (inputs_embeds is None):
            raise ValueError("Specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids.long())

        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if use_cache and past_key_values is None:
            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))

        batch_size, seq_len, _ = inputs_embeds.shape
        if encoder_hidden_states is None:
            raise ValueError("encoder_hidden_states must be provided")

        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0
        if cache_position is None:
            cache_position = torch.arange(
                past_key_values_length, past_key_values_length + seq_len, device=inputs_embeds.device
            )
        if decoder_attention_mask is not None and decoder_attention_mask.dim() == 2:
            expected_len = past_key_values_length + seq_len
            if decoder_attention_mask.shape[-1] != expected_len:
                if decoder_attention_mask.shape[-1] > expected_len:
                    decoder_attention_mask = decoder_attention_mask[:, -expected_len:]
                else:
                    decoder_attention_mask = F.pad(
                        decoder_attention_mask,
                        (expected_len - decoder_attention_mask.shape[-1], 0),
                        value=1,
                    )

        rotary_freqs = None
        if self.rotary is not None:
            rotary_freqs = self.rotary(seq_len, inputs_embeds.device, positions=cache_position)

        attn_implementation = getattr(self.config, "_attn_implementation", None) or "eager"
        if output_attentions and attn_implementation != "eager":
            logger.warning_once(
                "MoonshineStreaming attention does not support `output_attentions=True` with "
                f"`attn_implementation={attn_implementation}`. Falling back to eager attention."
            )
            attn_implementation = "eager"

        self_attn_mask = None
        if "flash_attention" in attn_implementation:
            if decoder_attention_mask is not None and (decoder_attention_mask == 0).any():
                self_attn_mask = decoder_attention_mask
        elif attn_implementation == "sdpa":
            self_attn_mask = _prepare_4d_causal_attention_mask_for_sdpa(
                decoder_attention_mask,
                inputs_embeds.shape[:-1],
                inputs_embeds,
                past_key_values_length=past_key_values_length,
            )
        else:
            self_attn_mask = _prepare_4d_causal_attention_mask(
                decoder_attention_mask,
                inputs_embeds.shape[:-1],
                inputs_embeds,
                past_key_values_length=past_key_values_length,
            )

        cross_attn_mask = None
        if encoder_attention_mask is not None:
            if "flash_attention" in attn_implementation:
                cross_attn_mask = encoder_attention_mask if (encoder_attention_mask == 0).any() else None
            elif attn_implementation == "sdpa":
                cross_attn_mask = _prepare_4d_attention_mask_for_sdpa(
                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_len
                )
            else:
                cross_attn_mask = _prepare_4d_attention_mask(
                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_len
                )

        hidden_states = (inputs_embeds,) if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_cross_attns = () if output_attentions else None
        x = inputs_embeds
        for layer in self.layers:
            x, self_attn_weights, cross_attn_weights = layer(
                x,
                encoder_hidden_states,
                cross_attn_mask=cross_attn_mask,
                rotary_freqs=rotary_freqs,
                self_attn_mask=self_attn_mask,
                output_attentions=output_attentions,
                past_key_values=past_key_values,
                cache_position=cache_position,
            )
            if output_hidden_states:
                hidden_states = hidden_states + (x,)
            if output_attentions:
                all_self_attns = all_self_attns + (self_attn_weights,)
                all_cross_attns = all_cross_attns + (cross_attn_weights,)

        x = self.final_norm(x)
        x = self.final_dropout(x)

        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=x,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=hidden_states,
            attentions=all_self_attns,
            cross_attentions=all_cross_attns,
        )


class MoonshineStreamingPreTrainedModel(PreTrainedModel):
    config_class = MoonshineStreamingConfig
    base_model_prefix = "model"
    main_input_name = "input_values"
    input_modalities = "audio"
    _supports_flash_attn = True
    _supports_sdpa = True

    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor) -> torch.LongTensor:
        frame_len = int(round(self.config.sample_rate * self.config.frame_ms / 1000.0))
        output_lengths = input_lengths // frame_len
        output_lengths = (output_lengths - 1) // 2 + 1
        output_lengths = (output_lengths - 1) // 2 + 1
        return output_lengths


class MoonshineStreamingModel(MoonshineStreamingPreTrainedModel):
    """
    The bare MoonshineStreaming encoder-decoder model outputting raw hidden-states without any specific head on top.

    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Args:
        config ([`MoonshineStreamingConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.

    Example:
        ```python
        >>> import torch
        >>> from transformers import MoonshineStreamingModel

        >>> model = MoonshineStreamingModel.from_pretrained("UsefulSensors/moonshine-streaming-tiny")

        >>> # Generate random audio input (batch_size=1, 16000 samples = 1 second of audio at 16kHz)
        >>> input_values = torch.randn(1, 16000)
        >>> decoder_input_ids = torch.tensor([[1]])  # Start token

        >>> outputs = model(input_values=input_values, decoder_input_ids=decoder_input_ids)
        >>> last_hidden_state = outputs.last_hidden_state
        ```
    """

    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__(config)
        self.encoder = MoonshineStreamingEncoder(config)
        self.adapter = MoonshineStreamingContextAdapter(config)
        self.decoder = MoonshineStreamingDecoder(config)
        self.post_init()

    def get_input_embeddings(self) -> nn.Module:
        return self.decoder.embed_tokens

    def set_input_embeddings(self, value: nn.Module):
        self.decoder.embed_tokens = value

    def forward(
        self,
        input_values: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[BaseModelOutput] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        decoder_inputs_embeds: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.Tensor] = None,
        use_cache: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Seq2SeqModelOutput:
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if encoder_outputs is None:
            if input_values is None:
                raise ValueError("input_values must be provided when encoder_outputs is None")
            # Pass raw audio directly to encoder (preprocessing is now handled internally)
            encoder_outputs = self.encoder(
                input_values,
                attention_mask=attention_mask,
                output_hidden_states=output_hidden_states,
                output_attentions=output_attentions,
            )
            # Get encoder attention mask from encoder's preprocessing
            if attention_mask is not None:
                lengths = attention_mask.sum(-1).to(dtype=torch.long)
                encoder_lengths = self._get_feat_extract_output_lengths(lengths)
                seq_len = encoder_outputs.last_hidden_state.shape[1]
                encoder_attention_mask = torch.arange(
                    seq_len, device=encoder_outputs.last_hidden_state.device
                ) < encoder_lengths.unsqueeze(1)
        else:
            if not isinstance(encoder_outputs, BaseModelOutput):
                encoder_outputs = BaseModelOutput(
                    last_hidden_state=encoder_outputs[0],
                    hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
                    attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
                )
            if encoder_attention_mask is None and attention_mask is not None:
                lengths = attention_mask.sum(-1).to(dtype=torch.long)
                encoder_lengths = self._get_feat_extract_output_lengths(lengths)
                seq_len = encoder_outputs.last_hidden_state.shape[1]
                encoder_attention_mask = torch.arange(
                    seq_len, device=encoder_outputs.last_hidden_state.device
                ) < encoder_lengths.unsqueeze(1)
            elif encoder_attention_mask is not None:
                encoder_attention_mask = encoder_attention_mask.to(dtype=torch.bool)

        encoder_hidden_states = encoder_outputs.last_hidden_state
        if encoder_attention_mask is not None:
            encoder_lengths = encoder_attention_mask.sum(-1)
        else:
            encoder_lengths = torch.full(
                (encoder_hidden_states.shape[0],),
                encoder_hidden_states.shape[1],
                dtype=torch.long,
                device=encoder_hidden_states.device,
            )

        adapted_states = self.adapter(encoder_hidden_states, encoder_lengths)

        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            inputs_embeds=decoder_inputs_embeds,
            encoder_hidden_states=adapted_states,
            encoder_attention_mask=encoder_attention_mask,
            decoder_attention_mask=decoder_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
        )

        if not return_dict:
            return Seq2SeqModelOutput(
                last_hidden_state=decoder_outputs.last_hidden_state,
                past_key_values=decoder_outputs.past_key_values,
                decoder_hidden_states=decoder_outputs.hidden_states,
                decoder_attentions=decoder_outputs.attentions,
                cross_attentions=decoder_outputs.cross_attentions,
                encoder_last_hidden_state=encoder_outputs.last_hidden_state,
                encoder_hidden_states=encoder_outputs.hidden_states,
                encoder_attentions=encoder_outputs.attentions,
            ).to_tuple()

        return Seq2SeqModelOutput(
            last_hidden_state=decoder_outputs.last_hidden_state,
            past_key_values=decoder_outputs.past_key_values,
            decoder_hidden_states=decoder_outputs.hidden_states,
            decoder_attentions=decoder_outputs.attentions,
            cross_attentions=decoder_outputs.cross_attentions,
            encoder_last_hidden_state=encoder_outputs.last_hidden_state,
            encoder_hidden_states=encoder_outputs.hidden_states,
            encoder_attentions=encoder_outputs.attentions,
        )


def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):
    """
    Shift input ids one token to the right.
    """
    shifted_input_ids = input_ids.new_zeros(input_ids.shape)
    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
    shifted_input_ids[:, 0] = decoder_start_token_id

    if pad_token_id is None:
        raise ValueError("self.model.config.pad_token_id has to be defined.")
    # replace possible -100 values in labels by `pad_token_id`
    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)

    return shifted_input_ids


class MoonshineStreamingForConditionalGeneration(MoonshineStreamingPreTrainedModel, GenerationMixin):
    """
    The MoonshineStreaming model with a language modeling head for speech-to-text transcription.

    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Args:
        config ([`MoonshineStreamingConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.

    Example:
        ```python
        >>> import torch
        >>> from transformers import AutoProcessor, MoonshineStreamingForConditionalGeneration
        >>> from datasets import load_dataset

        >>> processor = AutoProcessor.from_pretrained("UsefulSensors/moonshine-streaming-tiny")
        >>> model = MoonshineStreamingForConditionalGeneration.from_pretrained("UsefulSensors/moonshine-streaming-tiny")

        >>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:1]")
        >>> inputs = processor(ds[0]["audio"]["array"], return_tensors="pt")

        >>> generated_ids = model.generate(**inputs, max_new_tokens=100)
        >>> transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        >>> print(transcript)
        'Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'
        ```
    """

    _tied_weights_keys = {"lm_head.weight": "model.decoder.embed_tokens.weight"}

    def __init__(self, config: MoonshineStreamingConfig):
        super().__init__(config)
        self.model = MoonshineStreamingModel(config)
        self.lm_head = nn.Linear(config.decoder_dim, config.vocab_size, bias=False)
        self.post_init()

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def get_input_embeddings(self) -> nn.Module:
        return self.model.get_input_embeddings()

    def _prepare_encoder_decoder_kwargs_for_generation(
        self,
        inputs_tensor: torch.Tensor,
        model_kwargs,
        model_input_name: Optional[str],
        generation_config,
    ):
        del model_input_name
        attention_mask = model_kwargs.get("attention_mask", None)

        # Pass raw audio directly to encoder (preprocessing is now handled internally)
        encoder_outputs = self.model.encoder(
            input_values=inputs_tensor,
            attention_mask=attention_mask,
            output_attentions=generation_config.output_attentions,
            output_hidden_states=generation_config.output_hidden_states,
            return_dict=True,
        )

        # Compute encoder attention mask from input lengths
        if attention_mask is not None:
            lengths = attention_mask.sum(-1).to(dtype=torch.long)
            encoder_lengths = self._get_feat_extract_output_lengths(lengths)
            seq_len = encoder_outputs.last_hidden_state.shape[1]
            encoder_attention_mask = torch.arange(
                seq_len, device=encoder_outputs.last_hidden_state.device
            ) < encoder_lengths.unsqueeze(1)
        else:
            encoder_attention_mask = None

        model_kwargs["encoder_outputs"] = encoder_outputs
        model_kwargs["encoder_attention_mask"] = encoder_attention_mask
        model_kwargs["attention_mask"] = encoder_attention_mask
        return model_kwargs

    def forward(
        self,
        input_values: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        decoder_input_ids: Optional[torch.Tensor] = None,
        decoder_attention_mask: Optional[torch.Tensor] = None,
        encoder_outputs: Optional[BaseModelOutput] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        decoder_inputs_embeds: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.Tensor] = None,
        use_cache: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Seq2SeqLMOutput:
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:
            decoder_input_ids = shift_tokens_right(
                labels, self.config.pad_token_id, self.config.decoder_start_token_id
            )
        if labels is not None:
            if use_cache:
                logger.warning("The `use_cache` argument is changed to `False` since `labels` is provided.")
            use_cache = False

        outputs = self.model(
            input_values=input_values,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            encoder_outputs=encoder_outputs,
            encoder_attention_mask=encoder_attention_mask,
            decoder_inputs_embeds=decoder_inputs_embeds,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
            return_dict=return_dict,
        )

        if return_dict:
            decoder_hidden = outputs.last_hidden_state
        else:
            decoder_hidden = outputs[0]

        logits = self.lm_head(decoder_hidden)
        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)

        if not return_dict:
            output = (logits,) + outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return Seq2SeqLMOutput(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            decoder_hidden_states=outputs.decoder_hidden_states,
            decoder_attentions=outputs.decoder_attentions,
            cross_attentions=outputs.cross_attentions,
            encoder_last_hidden_state=outputs.encoder_last_hidden_state,
            encoder_hidden_states=outputs.encoder_hidden_states,
            encoder_attentions=outputs.encoder_attentions,
        )


__all__ = [
    "MoonshineStreamingModel",
    "MoonshineStreamingPreTrainedModel",
    "MoonshineStreamingForConditionalGeneration",
]
